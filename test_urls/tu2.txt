Перечитайте абзац выше несколько раз, чтобы точно разобраться. Обратите внимание, что вложенный словарь в данном случае — это та же гистограмма, он помогает нам отслеживать звенья и частоту их появления в тексте относительно других слов. Надо заметить, что даже такая словарная база очень мала для надлежащей генерации текстов на естественном языке — она должна содержать более 20 000 слов, а лучше более 100 000. А еще лучше — более 500 000. Но давайте рассмотрим ту словарную базу, которая получилась у нас. Цепь Маркова в данном случае строится аналогично первому примеру — каждое следующее слово выбирается только на основании знаний о текущем слове, все остальные слова не учитываются. Но благодаря хранению в словаре данных о том, какие слова появляются чаще других, мы можем при выборе принять взвешенное решение. Давайте разберем конкретный пример: